<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>research!rsc: Yacc is Not Dead</title>
    <link rel="alternate" type="application/atom+xml" title="research!rsc - Atom" href="http://research.swtch.com/feed.atom" />
    
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
<script type="text/javascript" src="https://use.typekit.com/skm6yij.js"></script>
<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
<style>
  body {
    padding: 0;
    margin: 0;
    font-size: 100%;
    font-family: 'Minion Pro';
  }
  @media print {
    img {page-break-inside: avoid;}
    div.nosplit {page-break-inside: avoid;}
  }
  img.center {
    display: block;
    margin: 0 auto;
  }
  .pad {
    padding-top: 1em;
    padding-bottom: 1em;
  }
  a.anchor, a.back, a.footnote {
    color: black !important;
    text-decoration: none !important;
  }
  a.back {
    font-size: 50%;
  }
  @media print {
    a.back {display: none;}
  }
  .header {
    height: 1.25em;
    background-color: #dff;
    margin: 0;
    padding: 0.1em 0.1em 0.2em;
    border-top: 1px solid black;
    border-bottom: 1px solid #8ff;
  }
  .header h3 {
    margin: 0;
    padding: 0 2em;
    display: inline-block;
    padding-right: 2em;
    font-style: italic;
    font-size: 90%;
  }
  .rss {
    float: right;
    padding-top: 0.2em;
    padding-right: 2em;
    display: none;
  }
  .toc {
    margin-top: 2em;
  }
  .toc-title {
    font-family: "caflisch-script-pro";
    font-size: 300%;
    line-height: 50%;
  }
  .toc-subtitle {
    display: block;
    margin-bottom: 1em;
    font-size: 83%;
  }
  @media only screen and (max-width: 550px) { .toc-subtitle { display: none; } }
  .header h3 a {
    color: black;
  }
  .header h4 {
    margin: 0;
    padding: 0;
    display: inline-block;
    font-weight: normal;
    font-size: 83%;
  }
  @media only screen and (max-width: 550px) { .header h4 { display: none; } }
  .main {
    padding: 0 2em;
  }
  @media only screen and (max-width: 479px) { .article { font-size: 120%; } }
  .article h1 {
    text-align: center;
    font-size: 200%;
  }
  .copyright {
    font-size: 83%;
  }
  .subtitle {
      font-size: 65%;
  }
  .normal {
    font-size: medium;
    font-weight: normal;
  }
  .when {
    text-align: center;
    font-size: 100%;
    margin: 0;
    padding: 0;
  }
  .when p {
    margin: 0;
    padding: 0;
  }
  .article h2 {
    font-size: 125%;
    padding-top: 0.25em;
  }
  .article h3 {
    font-size: 100%;
  }
  pre {
    margin-left: 4em;
    margin-right: 4em;
  }
  pre, code {
    font-family: 'Inconsolata', monospace;
    font-size: 100%;
  }
  .footer {
    margin-top: 10px;
    font-size: 83%;
    font-family: sans-serif;
  }
  .comments {
    margin-top: 2em;
    background-color: #ffe;
    border-top: 1px solid #aa4;
    border-left: 1px solid #aa4;
    border-right: 1px solid #aa4;
  }
  .comments-header {
    padding: 0 5px 0 5px;
  }
  .comments-header p {
    padding: 0;
    margin: 3px 0 0 0;
  }
  .comments-body {
    padding: 5px 5px 5px 5px;
  }
  #plus-comments {
    border-bottom: 1px dotted #ccc;
  }
  .plus-comment {
    width: 100%;
    font-size: 14px;
    border-top: 1px dotted #ccc;
  }
  .me {
    background-color: #eec;
  }
  .plus-comment ul {
    margin: 0;
    padding: 0;
    list-style: none;
    width: 100%;
    display: inline-block;
  }
  .comment-when {
    color:#999;
    width:auto;
    padding:0 5px;
  }
  .old {
    font-size: 83%;
  }
  .plus-comment ul li {
    display: inline-block;
    vertical-align: top;
    margin-top: 5px;
    margin-bottom: 5px;
    padding: 0;
  }
  .plus-icon {
    width: 45px;
  }
  .plus-img {
    float: left;
    margin: 4px 4px 4px 4px;
    width: 32px;
    height: 32px;
  }
  .plus-comment p {
    margin: 0;
    padding: 0;
  }
  .plus-clear {
    clear: left;
  }
  .toc-when {
    font-size: 83%;
    color: #999;
  }
  .toc {
    list-style: none;
  }
  .toc li {
    margin-bottom: 0.5em;
  }
  .toc-head {
    margin-bottom: 1em !important;
    font-size: 117%;
  }
  .toc-summary {
    margin-left: 2em;
  }
  .favorite {
    font-weight: bold;
  }
  .article p, .article ol {
    line-height: 144%;
  }
  sup, sub {
    vertical-align: baseline;
    position: relative;
    font-size: 83%;
  }
  sup {
    bottom: 1ex;
  }
  sub {
    top: 0.8ex;
  }

  .main {
    position: relative;
    margin: 0 auto;
    padding: 0;
    width: 900px;
  }
  @media only screen and (min-width: 768px) and (max-width: 959px) { .main { width: 708px; } }
  @media only screen and (min-width: 640px) and (max-width: 767px) { .main { width: 580px; } }
  @media only screen and (min-width: 480px) and (max-width: 639px) { .main { width: 420px; } }
  @media only screen and (max-width: 479px) { .main { width: 300px; } }

</style>

  </head>
  <body>
    
<div class="header">
  <h3><a href="/">research!rsc</a></h3>
  <h4>Thoughts and links about programming,
    by <a href="https://swtch.com/~rsc/" rel="author">Russ Cox</a> </h4>
  <a class="rss" href="/feed.atom"><img src="/feed-icon-14x14.png" /></a>
</div>

    <div class="main">
      <div class="article">
        <h1>Yacc is Not Dead
        
        <div class="normal">
        <div class="when">
          
            Posted on Monday, December 6, 2010.
            
          
        </div>
        </div>
        </h1>
        
<p><p class=pp>The internet tubes have been filled with chatter
recently about a paper posted on arXiv by Matthew Might
and David Darais titled &ldquo;<a href="http://arxiv.org/abs/1010.5023">Yacc is dead.</a>&rdquo;
Unfortunately, much of the discussion seems to have
centered on whether people think yacc is or should be dead and not
on the technical merits of the paper.
</p>

<p class=pp>
Reports of yacc's death, at least in this case, are greatly exaggerated.
The paper starts by arguing against the &ldquo;cargo cult parsing&rdquo; of putting
together parsers by copying and pasting regular expressions from other sources
until they work.  Unfortunately, the paper ends up being an example of a
much worse kind of cargo cult: claiming a theoretical grounding
without having any real theoretical basis.  That's a pretty strong
criticism, but I think it's an important one, because the paper is 
repeating the same mistakes that led to <a href="http://swtch.com/~rsc/regexp/regexp1.html">widespread exponential time regular
expression matching.</a>
I'm going to spend the rest of this post explaining
what I mean.  But to do so, we must first review some history.</p>

<p class=pp>Grep and yacc are the two canonical examples from Unix of 
good theory&#8212;regular expressions and LR parsing&#8212;producing
useful software tools.  The good theory part is important, and
it's the reason that yacc isn't dead.</p>

<h3>Regular Expressions</h3>

<p class=pp>The first seminal paper about processing regular expressions
was Janus Brzozowski's 1964 CACM article &ldquo;<a href="http://portal.acm.org/citation.cfm?id=321249">Derivatives of regular expressions</a>.&rdquo;
That formulation was the foundation behind
Ken Thompson's clever on-the-fly regular expression compiler,
which he described in the 1968 CACM article &ldquo;<a href="http://portal.acm.org/citation.cfm?id=363387">Regular expression search algorithm</a>.&rdquo;
Thompson learned regular expressions from Brzozowski himself
while both were at Berkeley, and he credits the method in the 1968 paper.
The now-standard framing of Thompson's paper as an NFA construction
and the addition of caching states to produce a DFA were both
introduced later, <a href="http://swtch.com/~rsc/regexp/regexp2.html#attrib">by Aho and Ullman in their various textbooks</a>.
Like many primary sources, the original is different from the
textbook presentations of it.
(I myself am <a href="http://swtch.com/~rsc/regexp/regexp1.html">also guilty</a> of this particular misrepresentation.)</p>

<p class=pp>The 1964 theory had more to it than the 1968 paper used, though.
In particular, because each state computed by Thompson's code
was ephemeral&#8212;it only lasted during the processing of a single byte&#8212;there
was no need to apply the more aggressive transformations that
Brzozowski's paper included, like rewriting <code>x|x</code> into <code>x</code>.</p>

<p class=pp>A much more recent paper by Owens, Reppy, and Turon, titled
&ldquo;<a href="http://www.ccs.neu.edu/home/turon/re-deriv.pdf">Regular-expression derivatives reexamined,</a>&rdquo;
returned to the original Brzozowski paper and considered the
effect of using Brzozowski's simplification rules in a regular
expression engine that cached states (what textbooks call a DFA).
There, the simplification has the important benefit that it
reduces the number of distinct states, so that a lexer generator
like lex or a regular expression matcher like grep can reduce
the necessary memory footprint for a given pattern.
Because the implementation manipulates regular expressions
symbolically instead of the usual sets of graph nodes, it has a very
natural expression in most functional programming languages.
So it's more efficient and easier to implement: win-win.</p>

<h3>Yacc is dead?</h3>

<p class=pp>That seems a bit of a digression, but it is relevant to the 
&ldquo;Yacc is dead&rdquo; paper.  That paper, claiming inspiration from
the &ldquo;Regular-expression derivatives reexamined&rdquo; paper,
shows that since you can define derivatives of context free grammars,
you can frame context free parsing as taking successive derivatives.
That's definitely true, but not news.
Just as each input transition of an NFA can be described as
taking the derivative (with respect to the input character)
of the regular expression corresponding to the originating NFA state,
each shift transition in an LR(0) parsing table can be described
as taking the derivative (with respect to the shift symbol)
of the grammar corresponding to the originating LR(0) state.
A key difference, though, is that NFA states typically carry
no state with them and can have duplicates removed,
while a parser associates with each LR(0) parse states a parse stack,
making duplicate removal more involved.</p>

<h3>Linear vs Exponential Runtime</h3>

<p class=pp>The linear time guarantee of regular expression matching
is due to the fact that, because you can eliminate duplicate states
while processing, there is a finite bound on 
the number of NFA states that must be considered at any
point in the input.  If you can't eliminate duplicate LR(0)
states there is no such bound and no such linear time guarantee.</p>

<p class=pp>Yacc and other parser generators guarantee linear time
by requiring that the parsing state automata not have even
a hint of ambiguity: there must never be a
(state, input symbol) combination where multiple possible
next states must be explored.  In yacc, these possible
ambiguities are called shift/reduce or reduce/reduce conflicts.
The art of writing yacc grammars without these conflicts
is undeniably arcane, and yacc in particular does a bad
job of explaining what went wrong, but that is not inherent
to the approach.  When you do manage to write down a
grammar that is free of these conflicts, yacc rewards you
with two important guarantees.  
First, the language you have defined is unambiguous.
There are no inputs that can be interpreted in multiple ways.
Second, the generated parser will parse your 
input files in time linear in the length of the input,
even if they are not syntactically valid.
If you are, say, defining a programming language,
these are both very important properties.</p>

<p class=pp>The approach outlined in the &ldquo;Yacc is dead&rdquo; paper gives
up both of these properties.  The grammar you give the parser
can be ambiguous.  If so, it can have exponentially
many possible parses to consider: they might all succeed,
they might all fail, or maybe all but the very last one will fail.
This implies a worst-case running time exponential in the
length of the input.  The paper makes some hand-wavy claims
about returning parses lazily, so that if a program only cares
about the first one or two, it does not pay the cost of computing
all the others.  That's fine, but if there are no successful parses,
the laziness doesn't help.</p>

<p>
It's easy construct plausible examples that take exponential
time to reject an input.  Consider this grammar for unary sums:</p>

<pre class=indent>
S &#8594; T
T &#8594; T + T | N
N &#8594; 1
</pre>

<p class=pp>This grammar is ambiguous: it doesn't say whether 1+1+1 should be parsed
as (1+1)+1 or 1+(1+1).
Worse, as the input gets longer there are exponentially many possible parses,
<a href="http://oeis.org/A000081">approximately O(3<sup>n</sup>) of them</a>.
If the particular choice doesn't matter, then for
a well-formed input you could take the first parse, whatever it is,
and stop, not having wasted any time on the others.
But suppose the input has a syntax error, like
1+1+1+...+1+1+1++1.  The backtracking parser will try all the
possible parses for the long initial prefix just in case one of them
can be followed by the erroneous ++1.  
So when the user makes a typo, your program grinds to a halt.</p>

<p class=pp>For an example that doesn't involve invalid input,
we can use the fact that any regular expression can be translated
directly into a context free grammar.  If you translated 
<a href="http://swtch.com/~rsc/regexp/regexp1.html">a test case
that makes Perl's backtracking take exponential time before finding a match</a>
into a grammar, the translated case would make the backtracking
parsers in this paper take exponential time to find a match too.</p>

<p class=pp>Exponential run time is a
<a href="http://galleries.csail.mit.edu/main.php?g2_itemId=7333">great name for a
computer science lab's softball team</a> but in other contexts is best avoided.</p>

<h3>Approaching Ambiguity</h3>

<p class=pp>The &ldquo;Yacc is dead&rdquo; paper observes correctly that ambiguity
is a fact of life if you want to handle arbitrary context free grammars.
I mentioned above the benefit that if yacc accepts your grammar,
then (because yacc rejects all the ambiguous context free grammars, and then some),
you know your grammar is unambiguous.
I find this invaluable as a way to vet possible language syntaxes,
but it's not necessary for all contexts.
Sometimes you don't care, or you know the language is
ambiguous and want all the possibilities.  
Either way, you can do better than exponential time parsing.
Context free parsing algorithms like the <a href="http://en.wikipedia.org/wiki/CYK_algorithm">CYK algorithm</a> or the <a href="http://en.wikipedia.org/wiki/GLR_parser">Generalized LR parsing</a> can 
build a tree representing all possible parses in only O(n<sup>3</sup>) time.
That's not linear, but it's a far cry from exponential.</p>

<p class=pp>Another interesting approach to handling ambiguity is to
give up on context free grammars entirely.
Parsing expression grammars, which can be parsed
efficiently using <a href="http://pdos.csail.mit.edu/~baford/packrat/">packrat parsing</a>,
replace the alternation of context free grammars with
an ordered (preference) choice, removing any ambiguity
from the language and achieving linear time parsing.
This is in some ways the same approach yacc takes, but
some people find it easier to write parsing expression
grammars than yacc grammars.</p>

<p class=pp>These are basically the only two approaches to ambiguity in
context free grammars: either handle it (possible in polynomial time)
or restrict the possible input grammars to ensure linear time
parsing and as a consequence eliminate ambiguity.
The obvious third approach&#8212;accept all context free grammars
but identify and reject just the ambiguous ones&#8212;is an undecidable
problem, equivalent to <a href="http://www.ling.ed.ac.uk/~gpullum/loopsnoop.html">solving the halting problem</a>.</p>

<h3>Cargo cults</h3>

<p class=pp>The &ldquo;Yacc is dead&rdquo; paper begins with a scathing critique of the
practice of parsing context free languages with (not-really-)regular
expressions in languages like Perl, which Larry Wall apparently once referred to
as &ldquo;cargo cult parsing&rdquo; due to the heavy incidence of cut and paste
imitation and copying &ldquo;magic&rdquo; expressions.
The paper says that people abuse regular expressions
instead of turning to tools like yacc because
&ldquo;regular expressions are `WYSIWYG'&#8212;the language described is the language that gets matched&#8212;whereas parser-generators are WYSIWYGIYULR(k)&#8212;`what you see is what you get if you understand
LR(k).' &rdquo;</p>

<p class=pp>The paper goes on:
<blockquote>
To end cargo-cult parsing, we need a new approach to parsing that:
<ul><li>handles arbitrary context-free grammars;
<li>parses efficiently on average; and
<li>can be implemented as a library with little effort.
</ul>
</blockquote></p>

<p class=pp>Then the paper starts talking about the &ldquo;Regular-expression derivatives reexamined&rdquo; paper,
which really got me excited.
I hoped that just like returning to derivatives led to an
easy-to-implement and oh-by-the-way more efficient
approach to regular expression matching, I had high hopes
that the same would happen for context free parsing.
Instead, the paper takes one of the classic blunders in
regular expression matching&#8212;<a href="http://swtch.com/~rsc/regexp/regexp1.html">search via
exponential backtracking because the code is easier to write</a>&#8212;and
applies it to context free parsing.</p>

<p class=pp>Concretely, the paper fails at goal #2: &ldquo;parses efficiently on average.&rdquo;
Most arbitrary context-free grammars are ambiguous,
and most inputs are invalid, so most parses will take
exponential time exploring all the ambiguities before giving
up and declaring the input unparseable.
(Worse, the parser couldn't even parse an unambiguous S-expression
grammar efficiently without adding a special case.)</p>

<p class=pp>Instead of ending the supposed problem of cargo cult parsing
(a problem that I suspect is endemic mainly to Perl), the paper ends up being
a prime example of what Richard Feynman called &ldquo;<a href="http://en.wikipedia.org/wiki/Cargo_cult_science">cargo cult science</a>&rdquo;
(Larry Wall was riffing on Feynman's term),
in which a successful line of research is imitated
but without some key aspect that made the original succeed
(in this case, the theoretical foundation that gave the 
efficiency results), leading to a failed result.</p>

<p class=pp>That criticism aside, I must note that the <del>Haskell</del> Scala code in the paper
really does deliver on goals #1 and #3.  It handles arbitrary
context-free grammars, it takes little effort, and on top of that
it's very elegant code.
Like that <del>Haskell</del> Scala code, the C regular expression matcher
in <a href="http://www.ddj.com/architect/184410904">this article</a>
(also Chapter 9 of <a href="http://www.amazon.com/dp/020161586X"><i>The Practice of Programming</i></a>
and Chapter 1 of <a href="http://www.amazon.com/dp/0596510047"><i>Beautiful Code</i></a>) is clean and elegant.
Unfortunately, the use of backtracking in both 
and the associated exponential run time makes
the code much nicer to study than to use.</p>

<h3>Is yacc dead?</h3>

<p class=pp><a href="http://dinosaur.compilertools.net/">Yacc</a> is an old tool, and it's showing its age,
but as the Plan 9 manual says of <a href="http://plan9.bell-labs.com/magic/man2html/1/lex"><i>lex</i>(1)</a>, &ldquo;The asteroid to kill this dinosaur is still in orbit.&rdquo;
Even if you think yacc itself is unused (which is far from the truth),
the newer tools that provide compelling alternatives still embody its spirit.
<a href="http://www.gnu.org/software/bison/">GNU Bison</a> can optionally
generate a GLR parser
instead of an LALR(1) parsers,
though Bison still retains yacc's infuriating lack of detail
in error messages.
(I use an awk script to parse the bison.output file and
tell me what really went wrong.)
<a href="http://www.antlr.org/">ANTLR</a> is a more full featured
though less widespread take on the approach.
These tools and many others all have the guarantee that
if they tell you the grammar is unambiguous, they'll give you
a linear-time parser, and if not, they'll give you at worst a
cubic-time parser.
Computer science theory doesn't know a better way.
But any of these is better than an exponential time parser.</p>

<p class=pp>So no, yacc is not dead, even if it sometimes smells that way,
and even if many people wish it were.</p></p>





<div class="comments">
  <div class="comments-header old">
    <p>(Comments originally posted via Blogger.)</p>
  </div>
  <div class="comments-body">
    <div id="plus-comments">
 <div class="plus-comment"><ul><li class="plus-text">
    <p><a href='http://www.blogger.com/profile/11443533397239445174'>shaunxcode</a> <span class="comment-when">(December 6, 2010 8:48 AM)</span> I believe the parser code in the paper is actually written in Scala and not Haskell.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p><a href='http://gregturn.myopenid.com/'>gregturn</a> <span class="comment-when">(December 6, 2010 9:08 AM)</span> If you need something newer (and slicker) check out PLY (Python Lex Yacc). I reverse engineered a grammar using this in order to create a converter.<br /><br />http://www.dabeaz.com/ply/</p>
 </li></ul></div>
 <div class="plus-comment me"><ul><li class="plus-text">
    <p><a href="http://swtch.com/~rsc/">Russ Cox</a> <span class="comment-when">(December 6, 2010 10:04 AM)</span> @shaunxcode: Yes, Scala not Haskell.  Confused by the laziness.  The Haskell code is elsewhere.  Fixed, thanks.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p><a href='http://www.blogger.com/profile/11968251961041194013'>deeelwy</a> <span class="comment-when">(December 6, 2010 1:13 PM)</span> What do you think of Marpa by Jeffrey Kegler?  It&#39;s available at http://search.cpan.org/perldoc?Marpa , and ironically implemented in Perl.  It&#39;s based on Earley&#39;s parser, which seems like a really cool design.  See his blog at http://blogs.perl.org/users/jeffrey_kegler/archives.html for more details.  Also, the acknowledgments (http://search.cpan.org/~jkegl/Marpa-0.200000/lib/Marpa/Doc/Marpa.pod#ACKNOWLEDGMENTS) section of the documentation includes a link to Earley&#39;s paper Marpa&#39;s algorithm is based on.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p> <span class="comment-when">(December 6, 2010 1:26 PM)</span> I&#39;m a big Yacc fan, and I wouldn&#39;t call it dead, but I&#39;d say it&#39;s<br />basically unused for parsing.<br /><br />By &quot;parsing&quot; I mean, very broadly, the problem of extracting structure<br />from a steam of bytes---text processing.  This kind of parser is<br />something that programmers are writing all the time, and, in the vast<br />majority of cases, they do not use parser generators like Yacc to do<br />so.  Instead they are using regexp libraries and other tools.<br /><br />By &quot;unused,&quot; I mean that the ratio of the number of Yacc/ANTLR/Peg<br />grammars to the number of Awk/Perl/Python text processing scripts is<br />approximately zero, and this ratio is decreasing over time.  Yacc&#39;s<br />usage is vanishingly small.<br /><br />It&#39;s interesting to consider why that is, given the purported<br />advantages of the Yacc approach.  My own intuition is that the<br />languages that people want to parse simply cannot be expressed as<br />unambiguous context-free grammars.  To the extent that Yacc is used,<br />it is because Yacc, the tool, in fact does something subtly different<br />than simply parse according to a grammar.<br /><br />For example, many programming languages have if-then-else syntax, and<br />the usual grammar for if-then-else is ambiguous, leading to the<br />classic shift-reduce conflict.  There is a way to write an unambiguous<br />grammar for if-then-else, but in practice people don&#39;t bother with<br />this.  Instead, they just use the ambiguous grammar and rely on Yacc<br />to resolve the ambiguity by shifting instead of reducing.<br /><br />I think if you examine the Yacc grammars that people have written,<br />you&#39;ll find that most of them have shift-reduce or reduce-reduce<br />conflicts: they aren&#39;t LR(1) and are often ambiguous.  Conflicts get<br />resolved automatically by Yacc or by semi-magical annotations added by<br />the programmer.  These grammars are still useful, of course, but I for<br />one don&#39;t claim to understand them well.<br /><br />In theoretical terms, an LR(1) parser generator can take any<br />context-free grammar G, regardless of whether it is LR(1) or<br />ambiguous, and produce a parser P.  However, when there are conflicts<br />in G (the usual case), the language L(P) of the parser may NOT be the<br />same as the language L(G) of the grammar.  (I&#39;m not even sure that<br />L(P) is context-free in the general case.)<br /><br />Now if-then-else doesn&#39;t prove my intuition because it is still<br />context-free.  However, most programming languages have<br />context-sensitive features.  This includes C/C++ (because of<br />typedefs), Python and Haskell (significant indentation), Ruby<br />(here-documents), Perl (obvious), Javascript (optional line-ending<br />semicolons), Standard ML (user-defined infix operators).  There are<br />ways to push these through Yacc---the so-called &quot;lexer hacks&quot;---but<br />these are hardly theoretically satisfying.<br /><br />If &quot;The good theory part is important,&quot; then we should be working to<br />improve on Yacc, and, in particular, context-free languages may not be<br />the best starting point.<br /><br />[Posting for and with permission of Trevor Jim.  -rsc]</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p>Anonymous <span class="comment-when">(December 6, 2010 8:32 PM)</span> &gt;These are basically the only two approaches to ambiguity in context free grammars<br /><br />The Lua parser is different. The Lua grammer is ambiguous (wierd edge cases involving function calls with newlines) but it complains when the parser encounters an ambiguity. Not so much the &#39;accept grammers&#39; part, as it is a custom parser.<br /><br />Dunno if this approach could be generalised to parser generators, but it is intresting, as it means the designer can effectively ignore edge cases by making them illegal.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p>Anonymous <span class="comment-when">(December 6, 2010 8:33 PM)</span> I love your analysis. BTW, I notice at http://bit.ly/eSNpw1 that Spiewalk says he has posted a comment on your analysis, but the comment was rejected. In the interest of science, would you please publish his comment? Of course, I will also be interested in your response. FYI, Spiewalk&#39;s comment is lodged at http://bit.ly/gpVqbL.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p> <span class="comment-when">(December 6, 2010 8:39 PM)</span> So, I promised myself that I wasn&#39;t going to comment on this, but it turns out that I can&#39;t resist...<br /><br />There are a few errors in this post which I feel the need to address.  Specifically dealing with derivative parsing, the article makes the claim (and uses that claim to support several arguments) that the parsing algorithm outlined in the &quot;Yacc is dead&quot; paper is exponential due to backtracking.  This is not the case.  Granted, it&#39;s not so easy to see this due to a lack of formal complexity analysis in the paper, but the algorithm doesn&#39;t do *any* backtracking whatsoever.  Once it consumes a token, it moves on and never returns.<br /><br />However, the algorithm *is* exponential.  More specifically, the algorithm presented in the paper is O(k^n) where k is the number of distinct terminals used in the input text.  Actually, the bound is a bit less than that, but it&#39;s simpler just to say O(k^n).  The reason for this exponentiality has nothing to do with backtracking, it has to do with right recursion.  Consider the following grammar:<br /><br />X ::= X a<br />    | b X<br />    | c<br /><br />When we derive X with respect to b, the result will include *all* of X (underived), as well as *all* of X derived with respect to b.  If we continue deriving with alternations of a and b, the resulting derivative grammars grow exponentially with a factor of 2^n until all possible ascending substrings of the input stream have been generated and memoized.  However, by that point, the grammar is absolutely enormous.  And, as the derivation is linearly proportional to the size of the grammar, each successive derivative requires exponentially more time to complete.  Multiply that by n derivatives, and you have the upper-bound.<br /><br />Note that this has nothing to do with &quot;exploring all alternatives&quot; for a particular prefix, nor does it say anything about valid vs invalid input.  This grammar will be exponential on *any* input, valid or otherwise.<br /><br />Of course, whether or not the algorithm is backtracking has very little bearing on the main points of your analysis.  After all, it&#39;s still exponential, and exponentiality is bad.  So, it&#39;s really just splitting hairs, but I thought it was important to clarify.<br /><br />In other news, you make some hand-wavy claims about an &quot;exponential number of parses&quot; directly implying &quot;exponential runtime&quot;.  This is not strictly true.  Generalized parsing algorithms (which, ironically, you bring up) have been solving this problem for years by memoizing shared prefixes and suffixes of alternate parse trails (e.g. with Tomita&#39;s graph-structured stack).  It turns out that if you don&#39;t require all possible *results*, you can get a &quot;yes/no&quot; recognition of any given input string in O(n^3).  If you actually need every result though, then clearly it&#39;s going to be exponential (since there may be an exponential number of results, and you have to enumerate them all).  In practice, practical tools like SGLR solve this problem of exponentiality in the number of *results* by generating a shared packed parse forest, which is basically just a lazy set of parse results.  This, incidentally, touches on another slightly-incorrect statement in the article, which claims that GLR can &quot;build a tree representing all possible parses in O(n^3) time&quot;.  Technically, it&#39;s not building the whole tree.  If it were, then it would be exponential.  It&#39;s just building *one* tree and saving just enough information that it can build the rest if need-be.<br /><br />Critically though, for a proper O(n^3) generalized parsing algorithm like CYK, RINGLR (GLR isn&#39;t actually generalized), GLL and similar, both valid *and* invalid strings can be recognized (or rejected) in O(n^3) time.<br /><br />Anyway, the algorithm in the &quot;Yacc is dead&quot; paper *is* exponential, and most of the statements in the article are quite valid.  However, the analysis of generalized parsing in general (no pun intended) is somewhat misleading and casts undue aspersions on the field as a whole.</p>
 </li></ul></div>
 <div class="plus-comment me"><ul><li class="plus-text">
    <p><a href="http://swtch.com/~rsc/">Russ Cox</a> <span class="comment-when">(December 6, 2010 8:40 PM)</span> &gt; So, I promised myself that I wasn&#39;t going to comment on this, but it turns<br />&gt; out that I can&#39;t resist...<br />&gt;<br />&gt; There are a few errors in this post which I feel the need to address.<br />&gt; Specifically dealing with derivative parsing, the article makes the claim<br />&gt; (and uses that claim to support several arguments) that the parsing<br />&gt; algorithm outlined in the &quot;Yacc is dead&quot; paper is exponential due to<br />&gt; backtracking.  This is not the case.  Granted, it&#39;s not so easy to see this<br />&gt; due to a lack of formal complexity analysis in the paper, but the algorithm<br />&gt; doesn&#39;t do *any* backtracking whatsoever.  Once it consumes a token, it<br />&gt; moves on and never returns.<br />&gt; ...<br /><br />I looked at the paper again and I might agree, but it&#39;s hard to say.<br />The code doesn&#39;t actually show what is lazy and what is not.<br />I interpreted the sentence about laziness and recursive grammars<br />on the second page of Section 9 as saying that even the rest of<br />the derivative wasn&#39;t computed until the first part had been used up,<br />so that the lazy computation would be like a frozen backtracking<br />waiting to be unthreaded.  Your interpretation - that the derivative<br />is somewhat less lazy than I thought - seems equally plausible.<br />In that case the parser would be even less efficient than claimed,<br />since it would be doing more work than necessary to find the first<br />parse result.<br /><br />&gt; In other news, you make some hand-wavy claims about an &quot;exponential number<br />&gt; of parses&quot; directly implying &quot;exponential runtime&quot;.  This is not strictly<br />&gt; true.  Generalized parsing algorithms (which, ironically, you bring up) have<br />&gt; been solving this problem for years by memoizing shared prefixes and<br />&gt; suffixes of alternate parse trails (e.g. with Tomita&#39;s graph-structured<br />&gt; stack).  It turns out that if you don&#39;t require all possible *results*, you<br />&gt; can get a &quot;yes/no&quot; recognition of any given input string in O(n^3).  If you<br />&gt; actually need every result though, then clearly it&#39;s going to be exponential<br />&gt; (since there may be an exponential number of results, and you have to<br />&gt; enumerate them all).  In practice, practical tools like SGLR solve this<br />&gt; problem of exponentiality in the number of *results* by generating a shared<br />&gt; packed parse forest, which is basically just a lazy set of parse results.<br />&gt;<br />&gt; This, incidentally, touches on another slightly-incorrect statement in the<br />&gt; article, which claims that GLR can &quot;build a tree representing all possible<br />&gt; parses in O(n^3) time&quot;.  Technically, it&#39;s not building the whole tree.  If<br />&gt; it were, then it would be exponential.  It&#39;s just building *one* tree and<br />&gt; saving just enough information that it can build the rest if need-be.<br /><br />I think we agree.  I have used GLR parsers in the past that included<br />merge nodes to mark ambiguities in the resulting parse tree.<br />That is what I meant by &quot;a tree representing all possible parses<br />in O(n^3) time.&quot;  It can build *one* tree that contains *all* the information.<br /><br />On the other hand, if the API is to return a lazy stream of parse<br />trees that enumerates all the possible parse trees (as I believe<br />the API in the paper does) then any use of it to collect an ambiguous<br />parse is necessarily exponential just to get to the end of the stream.<br />It was that API that I intended when talking about exponential numbers<br />of parses implying exponential time.<br /><br />It was definitely not my intent to cast undue aspersions on generalized<br />parsing as a whole.  In fact quite the opposite: I was trying to say that<br />GLR is still a far better alternative than exponential time parsing.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p> <span class="comment-when">(December 6, 2010 8:42 PM)</span> &gt; In that case the parser would be even less efficient than claimed,<br />&gt; since it would be doing more work than necessary to find<br />&gt; the first parse result.<br /><br />I&#39;m not sure I follow on that score.  It&#39;s really just an eagerly-evaluated, breadth-first traversal of a top-down parse trail.  This indicates that it should require potentially less work to find the first recognition point than a standard depth-first top-down traversal (e.g. recursive descent).</p>
 </li></ul></div>
 <div class="plus-comment me"><ul><li class="plus-text">
    <p><a href="http://swtch.com/~rsc/">Russ Cox</a> <span class="comment-when">(December 6, 2010 8:43 PM)</span> [I have, with Daniel Spiewak&#39;s permission, pasted some emails we exchanged into the comment feed.  I apologize for any trouble people are having with Blogger.  I believe I have set up the blog to allow anonymous posting, though there is a 4096 character limit.  If you post a big comment and get a Request-URI too large error, your comment made it through, believe it or not.]</p>
 </li></ul></div>
 <div class="plus-comment me"><ul><li class="plus-text">
    <p><a href="http://swtch.com/~rsc/">Russ Cox</a> <span class="comment-when">(December 6, 2010 9:00 PM)</span> &gt; I&#39;m not sure I follow on that score. It&#39;s really just <br />&gt; an eagerly-evaluated, breadth-first traversal of<br />&gt; a top-down parse trail. This indicates that it<br />&gt; should require potentially less work to find the<br />&gt; first recognition point than a standard<br />&gt;  depth-first top-down traversal (e.g. recursive descent).<br /><br />Dropping down to regular expressions for a bit...<br /><br />In regular expression search, the backtracking has no memory and might end up tracing a particular subpath through the square many times before exhausting its possibilities.  Breadth-first traversal wins (in the worst case) not because of the order in which it visits things but because it guarantees not to duplicate effort.  The breadth-first traversal pays extra up front cost (maintaining all the secondary possibilities that often don&#39;t end up coming into play) to avoid the worst case possibility.  It can&#39;t keep up with backtracking&#39;s best case, because the best case is that backtracking cuts corners <i>and gets away with it</i>, not ever needing the work that it skipped.<br /><br />That is, the breadth-first traversal itself is typically a loss compared to the depth-first one (it does more work than strictly necessary) but it makes up for that by never doing the same work twice.<br /><br />An even faster search is to use the depth-first traversal but augment it with a bitmap of visited [string position, NFA state] pairs to avoid doing the same work twice.  RE2 does this for small searches (when the bitmap is not too expensive) and it beats the pants off of the breadth-first traversal.<br /><br />Moving back up to parsing...<br /><br />Everything I said about depth-first beating breadth-first when ignoring duplication of work still applies here.  But the really bad part is that, unlike in the regular expression case, the breadth first traversal has no compensating factor to push it ahead: it cannot collapse duplicate states, because the associated parse values may be different, so the breadth-first state grows without bound, to no useful purpose if the first choice ends up succeeding.<br /><br />A GLR parser *can* collapse the states and record a &quot;merge&quot; node or some such in the parse value, and in that case breadth-first traversal comes out ahead again.  But the Yacc is Dead parsing has no such merge nodes, so if it&#39;s doing an eager breadth-first traversal instead of a lazy depth-first one, it&#39;s computing all the possible parse trees in parallel before returning any of them, which would be what I called &quot;more work than necessary to find the first parse result&quot;.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p><a href='http://www.blogger.com/profile/15048903730060869199'>scw</a> <span class="comment-when">(December 7, 2010 10:56 PM)</span> It may also be of use to mention <a href="http://www.hwaci.com/sw/lemon/" rel="nofollow">LEMON</a> in the context of parser error checking; it seems to do a nice job.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p> <span class="comment-when">(December 22, 2010 8:58 PM)</span> Jim Trevor reportedly wrote, <br> <br><i>If &quot;The good theory part is important,&quot; then we should be working to<br />improve on Yacc, and, in particular, context-free languages may not be<br />the best starting point.</i> <br><br> after musing on why so much text-processing is done with RE&#39;s instead of yacc/bison/etc.<br>The answer may be in part that (1) RE&#39;s are baked into popular scripting languages with shallow learning curves and (2) no one has made the problem of using yacc easier than understanding LR.<br><br>But! you say, you cannot get linear time for free: the ability of yacc to detect ambiguity is the means and understanding LR is the price.  And LL systems like ANTLR simply invert the pros/cons.  That&#39;s great for flamewars but not real progress.  <br><br>This is precisely what the PEG folks are challenging.  Do we really need machines to understand CFG&#39;s?  Is that even a good idea?  We can debate that.  (And we do.) But if the answer to either question <b>might</b> be &quot;no&quot; for any useful case, then we need to look at PEGs and other systems which reject the premise of the necessity of CFG.  And now that PEG&#39;s can support mutual left-recursion (e.g. OMeta) and even higher-order left-recursion (thanks to SÃ©rgio Queiroz de Medeiros) we really can work on making parser generators better than yacc -- at least for some applications which happen to be very useful.</p>
 </li></ul></div>
 <div class="plus-comment"><ul><li class="plus-text">
    <p><a href='http://www.blogger.com/profile/05853572318439263672'>Rajeev</a> <span class="comment-when">(March 19, 2011 7:12 AM)</span> it was a good post.....http://www.hordelevelingguidewow.com/</p>
 </li></ul></div>
    </div>
  </div>
</div>


      </div>
      
      
    </div>

    
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-3319603-2");
pageTracker._initData();
pageTracker._trackPageview();
</script>

    
    
  </body>
</html>
















